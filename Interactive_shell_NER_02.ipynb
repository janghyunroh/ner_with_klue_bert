{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T09:44:13.362955Z",
     "start_time": "2019-11-27T09:44:07.902764Z"
    }
   },
   "outputs": [],
   "source": [
    "# from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "# import json\n",
    "# import pickle\n",
    "# import torch\n",
    "# from gluonnlp.data import SentencepieceTokenizer\n",
    "# from model.net import KobertCRF\n",
    "# from data_utils.utils import Config\n",
    "# from data_utils.vocab_tokenizer import Tokenizer\n",
    "# from data_utils.pad_sequence import keras_pad_fn\n",
    "# from pathlib import Path\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from kobert_tokenizer import KoBERTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderFromNamedEntitySequence():\n",
    "    def __init__(self, tokenizer, index_to_ner):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.index_to_ner = index_to_ner\n",
    "        \n",
    "\n",
    "    def __call__(self, text, list_of_input_ids, list_of_pred_ids):\n",
    "        # input_token = self.tokenizer.decode_token_ids(list_of_input_ids)[0]\n",
    "        # input_token = self.tokenizer.decode(list_of_input_ids[0])\n",
    "        # self.encodings = self.tokenizer(text, return_offsets_mapping=True, return_tensors='pt')\n",
    "\n",
    "        input_token = self.tokenizer.convert_ids_to_tokens(list_of_input_ids[0])\n",
    "        input_token = self.modify_tokens(input_token)\n",
    "\n",
    "        pred_ner_tag = [self.index_to_ner[pred_id] for pred_id in list_of_pred_ids[0]]\n",
    "\n",
    "        # print(input_token)\n",
    "        # print(pred_ner_tag)\n",
    "\n",
    "        # ----------------------------- parsing list_of_ner_word ----------------------------- #\n",
    "        list_of_ner_word = []\n",
    "        entity_word, entity_tag, prev_entity_tag = \"\", \"\", \"\"\n",
    "        for i, pred_ner_tag_str in enumerate(pred_ner_tag):\n",
    "            if \"B-\" in pred_ner_tag_str:\n",
    "                entity_tag = pred_ner_tag_str[-3:]\n",
    "\n",
    "                if prev_entity_tag != entity_tag and prev_entity_tag != \"\":\n",
    "                    list_of_ner_word.append({\"word\": entity_word.replace(\"▁\", \" \"), \"tag\": prev_entity_tag, \"prob\": None})\n",
    "\n",
    "                entity_word = input_token[i]\n",
    "                prev_entity_tag = entity_tag\n",
    "            elif \"I-\"+entity_tag in pred_ner_tag_str:\n",
    "                entity_word += input_token[i]\n",
    "            else:\n",
    "                if entity_word != \"\" and entity_tag != \"\":\n",
    "                    list_of_ner_word.append({\"word\":entity_word.replace(\"▁\", \" \"), \"tag\":entity_tag, \"prob\":None})\n",
    "                entity_word, entity_tag, prev_entity_tag = \"\", \"\", \"\"\n",
    "\n",
    "\n",
    "        # ----------------------------- parsing decoding_ner_sentence ----------------------------- #\n",
    "        decoding_ner_sentence = \"\"\n",
    "        is_prev_entity = False\n",
    "        prev_entity_tag = \"\"\n",
    "        is_there_B_before_I = False\n",
    "\n",
    "        for i, (token_str, pred_ner_tag_str) in enumerate(zip(input_token, pred_ner_tag)):\n",
    "            if i == 0 or i == len(pred_ner_tag)-1: # remove [CLS], [SEP]\n",
    "                continue\n",
    "            token_str = token_str.replace('▁', ' ')  # '▁' 토큰을 띄어쓰기로 교체\n",
    "\n",
    "            if 'B-' in pred_ner_tag_str:\n",
    "                if is_prev_entity is True:\n",
    "                    decoding_ner_sentence += ':' + prev_entity_tag+ '>'\n",
    "\n",
    "                if token_str[0] == ' ':\n",
    "                    token_str = list(token_str)\n",
    "                    token_str[0] = ' <'\n",
    "                    token_str = ''.join(token_str)\n",
    "                    decoding_ner_sentence += token_str\n",
    "                else:\n",
    "                    decoding_ner_sentence += '<' + token_str\n",
    "                is_prev_entity = True\n",
    "                prev_entity_tag = pred_ner_tag_str[-2:] # 첫번째 예측을 기준으로 하겠음\n",
    "                is_there_B_before_I = True\n",
    "\n",
    "            elif 'I-' in pred_ner_tag_str:\n",
    "                decoding_ner_sentence += token_str\n",
    "\n",
    "                if is_there_B_before_I is True: # I가 나오기전에 B가 있어야하도록 체크\n",
    "                    is_prev_entity = True\n",
    "            else:\n",
    "                if is_prev_entity is True:\n",
    "                    decoding_ner_sentence += ':' + prev_entity_tag+ '>' + token_str\n",
    "                    is_prev_entity = False\n",
    "                    is_there_B_before_I = False\n",
    "                else:\n",
    "                    decoding_ner_sentence += token_str\n",
    "\n",
    "        return list_of_ner_word, decoding_ner_sentence\n",
    "    \n",
    "    def modify_tokens(self, tokens):\n",
    "        # 수정된 토큰들을 저장할 리스트\n",
    "        modified_tokens = []\n",
    "\n",
    "        # 토큰 리스트를 순회하면서 조건에 따라 수정\n",
    "        for token in tokens:\n",
    "            if token.startswith('##'):\n",
    "                # '##'으로 시작하는 경우 '##' 제거\n",
    "                modified_tokens.append(token[2:])\n",
    "            elif token not in ['[CLS]', '[SEP]']:\n",
    "                # '[CLS]', '[SEP]'이 아닌 경우 앞에 '_' 추가\n",
    "                modified_tokens.append('▁' + token)\n",
    "            else:\n",
    "                # 그 외의 경우 (즉, '[CLS]', '[SEP]', '.') 원본 유지\n",
    "                modified_tokens.append(token)\n",
    "\n",
    "        # 결과 출력\n",
    "        # print(modified_tokens)\n",
    "        return modified_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    model_name_split = model_name.split('/')\n",
    "    model = []\n",
    "    model.append(torch.load(\"./models/klue_roberta-base.15_epochs.100_length.0_fold.pth\",map_location='cuda:0' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_token_and_model(select_plm, index_to_label):\n",
    "    if select_plm=='skt/kobert-base-v1':\n",
    "        temp_tokenizer = KoBERTTokenizer.from_pretrained(select_plm)\n",
    "    else:\n",
    "        temp_tokenizer = AutoTokenizer.from_pretrained(select_plm)\n",
    "    temp_model = AutoModelForTokenClassification.from_pretrained(select_plm,num_labels=len(index_to_label))\n",
    "    return temp_tokenizer, temp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_for_demo(raw_text, plm_model, bert_best, tokenizer, index_to_label):\n",
    "    with torch.no_grad():\n",
    "        encoding = tokenizer(raw_text,return_tensors='pt')\n",
    "        device = next(plm_model.parameters()).device\n",
    "        tokenize_text = tokenizer.tokenize(raw_text)\n",
    "        \n",
    "        x = encoding['input_ids']\n",
    "        x = x.to(device)\n",
    "        mask = encoding['attention_mask']\n",
    "        mask = mask.to(device)\n",
    "        predictions = None\n",
    "\n",
    "        for bert in bert_best:\n",
    "            # evaluation mode,\n",
    "            plm_model.eval()\n",
    "            # Declare model and load pre-trained weights.\n",
    "            plm_model.load_state_dict(bert,strict=False)\n",
    "\n",
    "            # Take feed-forward\n",
    "            y_hat=plm_model(x, attention_mask=mask).logits\n",
    "            if predictions is None:\n",
    "                predictions=y_hat\n",
    "            else:\n",
    "                predictions+=y_hat\n",
    "            \n",
    "    prediction = predictions /5.\n",
    "    prediction = F.softmax(prediction, dim=-1)\n",
    "    indice = torch.argmax(prediction,dim=-1)\n",
    "    result = {}\n",
    "\n",
    "    list_of_input_ids = []\n",
    "    list_of_pred_ids = []\n",
    "    \n",
    "    # print(indice)\n",
    "    for i in range(1, len(indice[0])-1):\n",
    "        result[tokenize_text[i-1]] = index_to_label[int(indice[0][i])]\n",
    "\n",
    "    for i in range(0, len(indice[0])):\n",
    "        list_of_pred_ids.append(int(indice[0][i]))\n",
    "    \n",
    "    for i in range(0, len(x[0])):\n",
    "        list_of_input_ids.append(int(x[0][i]))\n",
    "    \n",
    "    return result, list_of_input_ids, list_of_pred_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model_name = 'klue/roberta-base'\n",
    "    saved_data = load_model(model_name)\n",
    "    bert_best = [model['bert'] for model in saved_data]\n",
    "    index_to_label = saved_data[0]['classes']\n",
    "\n",
    "    tokenizer, plm_model = load_token_and_model(model_name, index_to_label)\n",
    "\n",
    "    while(True):\n",
    "        input_text = input('Input: ')\n",
    "        if input_text == 'end':\n",
    "            break\n",
    "\n",
    "        result, list_of_input_ids, list_of_pred_ids = inference_for_demo(input_text, plm_model, bert_best, tokenizer, index_to_label)\n",
    "        list_of_input_ids = [list_of_input_ids]\n",
    "        list_of_pred_ids = [list_of_pred_ids]\n",
    "\n",
    "        decoder_from_res = DecoderFromNamedEntitySequence(tokenizer=tokenizer, index_to_ner=index_to_label)\n",
    "        list_of_ner_word, decoding_ner_sentence = decoder_from_res(text =input_text, list_of_input_ids=list_of_input_ids, list_of_pred_ids=list_of_pred_ids)\n",
    "        print(\"Output:\", decoding_ner_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  아름다운\n",
      "Output:  아름당누\n",
      "Output:  <1998 . 03 . 012\n",
      "Output:  <방사선과 전문의:JB> 또는 촬영을 의뢰한 정형외과 의사와 협의하여 즉시 적절한 대응조치를 취하여야 하고 또 촬영중 전문의의 판독과정을 거침으로써 조영제의 주입이 필요한 최소한에 그치도록 하여야 할 주의의무가 있음에도 불구하고 이를 게을리\n",
      "Output: \n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crf-ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
